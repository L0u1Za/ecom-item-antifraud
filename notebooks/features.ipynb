{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4f24c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "import warnings\n",
    "import math\n",
    "\n",
    "df_train = pd.read_csv('../data/train.csv', index_col=0)\n",
    "df_val = pd.read_csv('../data/val.csv', index_col=0)\n",
    "df_test = pd.read_csv('../data/test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e03053ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luiza\\AppData\\Local\\Temp\\ipykernel_28464\\1504901832.py:5: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  with initialize(config_path=\"../src/config\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing:\n",
      "  text:\n",
      "    apply_cleaning: true\n",
      "    apply_lemmatization: true\n",
      "    add_fraud_indicators: true\n",
      "    max_length: 1024\n",
      "    nltk_data_dir: ${oc.env:NLTK_DATA,${hydra:runtime.cwd}/nltk_data}\n",
      "  image:\n",
      "    size:\n",
      "    - 224\n",
      "    - 224\n",
      "    augmentations:\n",
      "    - transform:\n",
      "        _target_: torchvision.transforms.RandomResizedCrop\n",
      "        size: 224\n",
      "        scale:\n",
      "        - 0.8\n",
      "        - 1.0\n",
      "      probability: 0.5\n",
      "    - transform:\n",
      "        _target_: torchvision.transforms.RandomHorizontalFlip\n",
      "        p: 0.5\n",
      "      probability: 0.1\n",
      "    - transform:\n",
      "        _target_: torchvision.transforms.RandomRotation\n",
      "        degrees: 10\n",
      "      probability: 0.3\n",
      "    - transform:\n",
      "        _target_: torchvision.transforms.ColorJitter\n",
      "        brightness: 0.1\n",
      "        contrast: 0.1\n",
      "        saturation: 0.1\n",
      "        hue: 0.05\n",
      "      probability: 0.3\n",
      "    compute_clip_similarity: false\n",
      "    clip_model: openai/clip-vit-base-patch32\n",
      "  tabular:\n",
      "    categorical_cols:\n",
      "    - brand_name\n",
      "    - CommercialTypeName4\n",
      "    numerical_cols:\n",
      "    - rating_1_count\n",
      "    - rating_2_count\n",
      "    - rating_3_count\n",
      "    - rating_4_count\n",
      "    - rating_5_count\n",
      "    - comments_published_count\n",
      "    - photos_published_count\n",
      "    - videos_published_count\n",
      "    - PriceDiscounted\n",
      "    - item_time_alive\n",
      "    - item_count_fake_returns7\n",
      "    - item_count_fake_returns30\n",
      "    - item_count_fake_returns90\n",
      "    - item_count_sales7\n",
      "    - item_count_sales30\n",
      "    - item_count_sales90\n",
      "    - item_count_returns7\n",
      "    - item_count_returns30\n",
      "    - item_count_returns90\n",
      "    - GmvTotal7\n",
      "    - GmvTotal30\n",
      "    - GmvTotal90\n",
      "    - ExemplarAcceptedCountTotal7\n",
      "    - ExemplarAcceptedCountTotal30\n",
      "    - ExemplarAcceptedCountTotal90\n",
      "    - OrderAcceptedCountTotal7\n",
      "    - OrderAcceptedCountTotal30\n",
      "    - OrderAcceptedCountTotal90\n",
      "    - ExemplarReturnedCountTotal7\n",
      "    - ExemplarReturnedCountTotal30\n",
      "    - ExemplarReturnedCountTotal90\n",
      "    - ExemplarReturnedValueTotal7\n",
      "    - ExemplarReturnedValueTotal30\n",
      "    - ExemplarReturnedValueTotal90\n",
      "    - ItemVarietyCount\n",
      "    - ItemAvailableCount\n",
      "    - seller_time_alive\n",
      "    - desc_len\n",
      "    - desc_word_count\n",
      "    - name_len\n",
      "    - anomaly_score\n",
      "    - seller_total_items\n",
      "    - seller_total_sales\n",
      "    - seller_avg_return_rate\n",
      "    - fraud_desc_brand_names\n",
      "    - fraud_desc_messenger\n",
      "    - fraud_desc_mixed_alphabets\n",
      "    - fraud_desc_phone\n",
      "    - fraud_desc_repeated_chars\n",
      "    - fraud_desc_short_description\n",
      "    - fraud_desc_suspicious_keywords\n",
      "    - fraud_desc_suspicious_numbers\n",
      "    - fraud_desc_title_description_mismatch\n",
      "    - fraud_desc_unusual_punct\n",
      "    - fraud_desc_url\n",
      "    - fraud_title_brand_names\n",
      "    - fraud_title_messenger\n",
      "    - fraud_title_mixed_alphabets\n",
      "    - fraud_title_phone\n",
      "    - fraud_title_repeated_chars\n",
      "    - fraud_title_short_description\n",
      "    - fraud_title_suspicious_keywords\n",
      "    - fraud_title_suspicious_numbers\n",
      "    - fraud_title_title_description_mismatch\n",
      "    - fraud_title_unusual_punct\n",
      "    scaling: standard\n",
      "model:\n",
      "  model:\n",
      "    text:\n",
      "      enabled: true\n",
      "      name: cointegrated/rubert-tiny2\n",
      "      pretrained: true\n",
      "      projection_dim: 256\n",
      "      dropout: 0.3\n",
      "    image:\n",
      "      enabled: true\n",
      "      name: efficientnet_b0\n",
      "      pretrained: true\n",
      "      projection_dim: 256\n",
      "      dropout: 0.3\n",
      "    tabular:\n",
      "      enabled: true\n",
      "      categories: []\n",
      "      num_continuous: 0\n",
      "      dim: 32\n",
      "      projection_dim: 128\n",
      "      depth: 6\n",
      "      heads: 8\n",
      "      attn_dropout: 0.1\n",
      "      ff_dropout: 0.1\n",
      "    classifier:\n",
      "      hidden_dim: 512\n",
      "      dropout: 0.5\n",
      "    modality_dropout: 0.1\n",
      "    cache_dir: cache\n",
      "  fusion:\n",
      "    _target_: models.fusion.fusion.Fusion\n",
      "    type: early\n",
      "    output_dim: 256\n",
      "    dropout: 0.3\n",
      "training:\n",
      "  epochs: 20\n",
      "  resume_training: false\n",
      "  checkpoint_path: null\n",
      "  batch_size: 128\n",
      "  criterion:\n",
      "    _target_: torch.nn.BCEWithLogitsLoss\n",
      "    pos_weight: 15\n",
      "  optimizer:\n",
      "    _target_: torch.optim.Adam\n",
      "    lr: 0.0001\n",
      "    weight_decay: 1.0e-05\n",
      "    betas:\n",
      "    - 0.9\n",
      "    - 0.999\n",
      "  scheduler:\n",
      "    _target_: torch.optim.lr_scheduler.ReduceLROnPlateau\n",
      "    mode: max\n",
      "    factor: 0.5\n",
      "    patience: 3\n",
      "  validation:\n",
      "    threshold: 0.5\n",
      "experiment:\n",
      "  name: baseline\n",
      "  tags:\n",
      "  - fraud-detection\n",
      "  - multimodal\n",
      "  wandb:\n",
      "    enabled: true\n",
      "    project: fraud-detection\n",
      "    entity: luizanigogosova\n",
      "  data:\n",
      "    train_path: data/train.csv\n",
      "    val_path: data/val.csv\n",
      "    test_path: data/test.csv\n",
      "    train_images_path: data/ml_ozon_counterfeit_train_images/ml_ozon_сounterfeit_train_images\n",
      "    test_images_path: data/ml_ozon_сounterfeit_test_images/ml_ozon_сounterfeit_test_images\n",
      "    num_workers: 4\n",
      "  checkpointing:\n",
      "    dir: checkpoints\n",
      "    save_best: true\n",
      "    save_last: true\n",
      "    save_frequency: 10\n",
      "    save_frequency_enabled: true\n",
      "project:\n",
      "  name: fraud-detection\n",
      "  seed: 42\n",
      "  output_dir: ${hydra:runtime.output_dir}\n",
      "data_preparation:\n",
      "  train_path: data/ml_ozon_train.csv\n",
      "  test_path: data/ml_ozon_test.csv\n",
      "  output_dir: data\n",
      "processors:\n",
      "  tabular_processor_path: data/tabular_processor.pkl\n",
      "inference:\n",
      "  model_path: data/baseline_best.pt\n",
      "  output_path: data/submission.csv\n",
      "  threshold: 0.5\n",
      "  pr_curve_path: data/pr_curve.png\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "from hydra import compose, initialize\n",
    "\n",
    "with initialize(config_path=\"../src/config\"):\n",
    "    cfg = compose(config_name=\"config\")\n",
    "print(OmegaConf.to_yaml(cfg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "267efa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "\n",
    "class TabularProcessor:\n",
    "    def __init__(self,\n",
    "                 categorical_cols: List[str] = None,\n",
    "                 numerical_cols: List[str] = None,\n",
    "                 scaling: str = \"standard\"):\n",
    "        self.categorical_cols = categorical_cols or []\n",
    "        self.numerical_cols = numerical_cols or []\n",
    "        self.scaling = scaling\n",
    "\n",
    "        # Learned state\n",
    "        self.category_value_to_index: Dict[str, Dict[Any, int]] = {}\n",
    "        self.category_cardinalities: List[int] = []\n",
    "        self.num_stats: Dict[str, Dict[str, float]] = {}\n",
    "        self.num_categorical_features: int = 0\n",
    "        self.num_continuous_features: int = 0\n",
    "\n",
    "    def fit(self, df: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Fit encoders and scalers on training dataframe.\n",
    "\n",
    "        - Categorical columns: build value->index mapping with 0 reserved for unknown/missing\n",
    "        - Numerical columns: compute mean/std (standard) or min/max (minmax)\n",
    "        \"\"\"\n",
    "        # Fit categoricals\n",
    "        self.category_value_to_index = {}\n",
    "        self.category_cardinalities = []\n",
    "        for col in self.categorical_cols:\n",
    "            # Get unique values excluding NaNs\n",
    "            if col not in df.columns:\n",
    "                # still register unknown-only category\n",
    "                self.category_value_to_index[col] = {}\n",
    "                self.category_cardinalities.append(1)\n",
    "                continue\n",
    "            values = pd.Series(df[col]).astype(str)\n",
    "            # Include only non-null values\n",
    "            unique_values = pd.Index(values[values != 'nan'].unique())\n",
    "            # Reserve 0 for unknown\n",
    "            mapping = {val: i + 1 for i, val in enumerate(unique_values)}\n",
    "            self.category_value_to_index[col] = mapping\n",
    "            # cardinality includes index 0 for unknown\n",
    "            self.category_cardinalities.append(len(mapping) + 1)\n",
    "\n",
    "        # Fit numerical scaler\n",
    "        self.num_stats = {}\n",
    "        for col in self.numerical_cols:\n",
    "            if col not in df.columns:\n",
    "                self.num_stats[col] = {\"mean\": 0.0, \"std\": 1.0, \"min\": 0.0, \"max\": 1.0}\n",
    "                continue\n",
    "            series = pd.to_numeric(df[col], errors='coerce')\n",
    "            if self.scaling == \"standard\":\n",
    "                mean = float(series.mean()) if not math.isnan(series.mean()) else 0.0\n",
    "                std = float(series.std(ddof=0)) if not math.isnan(series.std(ddof=0)) else 1.0\n",
    "                if std == 0.0:\n",
    "                    std = 1.0\n",
    "                self.num_stats[col] = {\"mean\": mean, \"std\": std}\n",
    "            elif self.scaling == \"minmax\":\n",
    "                min_v = float(series.min()) if not math.isnan(series.min()) else 0.0\n",
    "                max_v = float(series.max()) if not math.isnan(series.max()) else 1.0\n",
    "                if max_v == min_v:\n",
    "                    max_v = min_v + 1.0\n",
    "                self.num_stats[col] = {\"min\": min_v, \"max\": max_v}\n",
    "            else:\n",
    "                # No scaling\n",
    "                self.num_stats[col] = {}\n",
    "\n",
    "        # Store feature counts\n",
    "        self.num_categorical_features = len(self.categorical_cols)\n",
    "        self.num_continuous_features = len(self.numerical_cols)\n",
    "\n",
    "    @property\n",
    "    def categories_cardinalities(self) -> List[int]:\n",
    "        return list(self.category_cardinalities)\n",
    "\n",
    "    @property\n",
    "    def num_continuous(self) -> int:\n",
    "        return len(self.numerical_cols)\n",
    "\n",
    "    def _encode_category_value(self, col: str, value: Any) -> int:\n",
    "        mapping = self.category_value_to_index.get(col, {})\n",
    "        if pd.isna(value):\n",
    "            return 0\n",
    "        key = str(value)\n",
    "        return mapping.get(key, 0)\n",
    "\n",
    "    def _scale_numeric_value(self, col: str, value: Any) -> float:\n",
    "        if value is None or (isinstance(value, float) and math.isnan(value)):\n",
    "            # impute with mean (standard) or min (minmax) or 0\n",
    "            if self.scaling == \"standard\":\n",
    "                return float(self.num_stats.get(col, {}).get(\"mean\", 0.0))\n",
    "            elif self.scaling == \"minmax\":\n",
    "                return float(self.num_stats.get(col, {}).get(\"min\", 0.0))\n",
    "            else:\n",
    "                return 0.0\n",
    "        try:\n",
    "            v = float(value)\n",
    "        except Exception:\n",
    "            v = 0.0\n",
    "        if self.scaling == \"standard\":\n",
    "            mean = self.num_stats.get(col, {}).get(\"mean\", 0.0)\n",
    "            std = self.num_stats.get(col, {}).get(\"std\", 1.0)\n",
    "            return (v - mean) / std\n",
    "        if self.scaling == \"minmax\":\n",
    "            min_v = self.num_stats.get(col, {}).get(\"min\", 0.0)\n",
    "            max_v = self.num_stats.get(col, {}).get(\"max\", 1.0)\n",
    "            return (v - min_v) / (max_v - min_v)\n",
    "        return v\n",
    "\n",
    "    def __call__(self, row: Dict[str, Any]) -> np.array:\n",
    "        # Categorical indices\n",
    "        cat_indices: List[int] = []\n",
    "        for col in self.categorical_cols:\n",
    "            val = row.get(col, None)\n",
    "            idx = self._encode_category_value(col, val)\n",
    "            cat_indices.append(idx)\n",
    "\n",
    "        # Continuous values\n",
    "        cont_values: List[float] = []\n",
    "        for col in self.numerical_cols:\n",
    "            val = row.get(col, None)\n",
    "            scaled = self._scale_numeric_value(col, val)\n",
    "            cont_values.append(float(scaled))\n",
    "\n",
    "        categorical = np.array(cat_indices)\n",
    "        continuous = np.array(cont_values) if cont_values else np.zeros(0)\n",
    "\n",
    "        return np.concat([categorical, continuous])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2924c960",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = TabularProcessor(categorical_cols=list(cfg.preprocessing.tabular.categorical_cols),\n",
    "            numerical_cols=list(cfg.preprocessing.tabular.numerical_cols),\n",
    "            scaling=cfg.preprocessing.tabular.get('scaling', 'standard'))\n",
    "\n",
    "processor.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e89e7979",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = df_train.drop(columns='resolution'), df_train['resolution']\n",
    "X_val, y_val = df_val.drop(columns='resolution'), df_val['resolution']\n",
    "X_test = df_test.copy()\n",
    "X_train = np.stack(X_train.apply(processor, axis=1))\n",
    "X_val = np.stack(X_val.apply(processor, axis=1))\n",
    "X_test = np.stack(X_test.apply(processor, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57f130aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.9712\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98     36830\n",
      "           1       0.90      0.64      0.75      2610\n",
      "\n",
      "    accuracy                           0.97     39440\n",
      "   macro avg       0.94      0.82      0.87     39440\n",
      "weighted avg       0.97      0.97      0.97     39440\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "val_pred = model.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, val_pred)\n",
    "\n",
    "print(f\"Validation accuracy: {val_accuracy:.4f}\")\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_val, val_pred))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4efaf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# 2. Compute SHAP values using TreeExplainer\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_train)  # returns per-sample contribution per feature :contentReference[oaicite:0]{index=0}\n",
    "\n",
    "# 3. Calculate global importance: mean absolute SHAP value per feature\n",
    "feature_names = X_train.columns\n",
    "# If shap_values is a 2D array (num_samples × num_features)\n",
    "importance = np.abs(shap_values).mean(axis=0)\n",
    "feature_importance = pd.Series(importance, index=feature_names).sort_values(ascending=False)\n",
    "\n",
    "\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7637ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Visualize top features\n",
    "shap.summary_plot(shap_values, X_train)      # Beeswarm plot :contentReference[oaicite:1]{index=1}\n",
    "shap.plots.bar(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a4a93425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.5501018\ttest: 0.5507720\tbest: 0.5507720 (0)\ttotal: 36.2ms\tremaining: 18s\n",
      "100:\tlearn: 0.1187064\ttest: 0.1232029\tbest: 0.1232029 (100)\ttotal: 3.59s\tremaining: 14.2s\n",
      "200:\tlearn: 0.1012097\ttest: 0.1071754\tbest: 0.1071754 (200)\ttotal: 8.03s\tremaining: 11.9s\n",
      "300:\tlearn: 0.0908517\ttest: 0.0985068\tbest: 0.0985068 (300)\ttotal: 13.6s\tremaining: 8.96s\n",
      "400:\tlearn: 0.0839749\ttest: 0.0938315\tbest: 0.0938275 (399)\ttotal: 18s\tremaining: 4.46s\n",
      "499:\tlearn: 0.0784132\ttest: 0.0901725\tbest: 0.0901725 (499)\ttotal: 22.3s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.09017252521\n",
      "bestIteration = 499\n",
      "\n",
      "Validation accuracy: 0.9682\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98     36830\n",
      "           1       0.83      0.65      0.73      2610\n",
      "\n",
      "    accuracy                           0.97     39440\n",
      "   macro avg       0.90      0.82      0.86     39440\n",
      "weighted avg       0.97      0.97      0.97     39440\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "model = CatBoostClassifier(\n",
    "    iterations=500,\n",
    "    learning_rate=0.1,\n",
    "    depth=6,\n",
    "    random_seed=42,\n",
    "    verbose=100\n",
    ")\n",
    "model.fit(X_train, y_train, eval_set=(X_val, y_val), use_best_model=True)\n",
    "\n",
    "val_pred = model.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, val_pred)\n",
    "\n",
    "print(f\"Validation accuracy: {val_accuracy:.4f}\")\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_val, val_pred))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "409b0fa1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 25\u001b[0m\n\u001b[0;32m     12\u001b[0m catboost_model \u001b[38;5;241m=\u001b[39m CatBoostClassifier(\n\u001b[0;32m     13\u001b[0m     random_seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[0;32m     14\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     17\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[0;32m     18\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mcatboost_model,\n\u001b[0;32m     19\u001b[0m     param_grid\u001b[38;5;241m=\u001b[39mparam_grid,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     23\u001b[0m )\n\u001b[1;32m---> 25\u001b[0m grid_search\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m, y_train)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest parameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_search\u001b[38;5;241m.\u001b[39mbest_params_)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest CV score:\u001b[39m\u001b[38;5;124m\"\u001b[39m, grid_search\u001b[38;5;241m.\u001b[39mbest_score_)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'iterations': [300, 500],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'depth': [4, 6, 8],\n",
    "    'l2_leaf_reg': [1, 3, 5, 7],\n",
    "    'border_count': [32, 64, 128]\n",
    "}\n",
    "\n",
    "catboost_model = CatBoostClassifier(\n",
    "    random_seed=42,\n",
    "    verbose=100\n",
    ")\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=catboost_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring='f1_macro',  # или 'accuracy', 'roc_auc' — по задаче\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best CV score:\", grid_search.best_score_)\n",
    "\n",
    "# Используем лучшие параметры для финального обучения\n",
    "best_model = grid_search.best_estimator_\n",
    "val_pred = best_model.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, val_pred)\n",
    "\n",
    "print(f\"Validation accuracy: {val_accuracy:.4f}\")\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_val, val_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5098ff91",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e8c3c734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Создан файл submission.csv с 22760 предсказаниями\n",
      "Распределение предсказаний:\n",
      "prediction\n",
      "0    21882\n",
      "1      878\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id': df_test.index,\n",
    "    'prediction': test_predictions\n",
    "})\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "\n",
    "print(f\"Создан файл submission.csv с {len(submission)} предсказаниями\")\n",
    "print(f\"Распределение предсказаний:\")\n",
    "print(submission['prediction'].value_counts())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0ee42a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
